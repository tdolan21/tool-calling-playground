{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ollama Web Scraping Tool\n",
    "\n",
    "This tool utilizes playwright and function calling with ollama to provide the llm with a webscraper that it can utilize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure ollama is installed \n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install ollama playwright beautifulsoup4 playwright-stealth\n",
    "\n",
    "# install required browsers\n",
    "% playwright install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the browsers installed, you can import the libraries and define the web scraper tool. This tool is using playwright due to its ability to render complex javascript and bypass standard detection systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import ollama\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from playwright_stealth import stealth_async\n",
    "from bs4 import BeautifulSoup\n",
    "import traceback\n",
    "\n",
    "# Web Scraper class definition\n",
    "class WebScraper:\n",
    "    def __init__(self, headless=True, browser_type=\"chromium\", chunk_size=256, max_tokens=1000):\n",
    "        self.headless = headless\n",
    "        self.browser_type = browser_type\n",
    "        self.chunk_size = chunk_size\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    async def scrape_page(self, url: str) -> str:\n",
    "        async with async_playwright() as p:\n",
    "            browser = await getattr(p, self.browser_type).launch(\n",
    "                headless=self.headless,\n",
    "                args=[\"--disable-gpu\", \"--no-sandbox\"]\n",
    "            )\n",
    "            context = await browser.new_context()\n",
    "            page = await context.new_page()\n",
    "\n",
    "            await stealth_async(page)\n",
    "            await page.goto(url)\n",
    "\n",
    "            html_content = await page.content()\n",
    "            await browser.close()\n",
    "        return html_content\n",
    "\n",
    "    def extract_titles_articles_links(self, raw_html: str) -> list:\n",
    "        soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "        extracted_data = []\n",
    "        \n",
    "        for article in soup.find_all(['article', 'section', 'div']):\n",
    "            title_tag = article.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "            link_tag = article.find('a', href=True)\n",
    "            content = article.get_text(separator=\"\\n\", strip=True)\n",
    "            \n",
    "            if title_tag and link_tag and content:\n",
    "                extracted_data.append({\n",
    "                    'title': title_tag.get_text(strip=True),\n",
    "                    'link': link_tag['href'],\n",
    "                    'content': content\n",
    "                })\n",
    "        \n",
    "        return extracted_data\n",
    "\n",
    "    async def query_page_content(self, url: str) -> dict:\n",
    "        raw_html = await self.scrape_page(url)\n",
    "        structured_data = {\n",
    "            \"url\": url,\n",
    "            \"extracted_data\": self.extract_titles_articles_links(raw_html),\n",
    "            \"raw_html\": raw_html\n",
    "        }\n",
    "        return structured_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once playwright retrieves all of the HTML on the given page, it will be cleaned with beautiful soup to make the response a bit more workable. Some models with small context windows may struggle with this method. Llama 3.1 8B works very well, but for example, command-r plus cannot use this tool because the HTML response will always be too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def query_web_scraper(url: str) -> dict:\n",
    "    scraper = WebScraper(headless=False)\n",
    "    return await scraper.query_page_content(url)\n",
    "\n",
    "async def write_raw_html_to_file(raw_html: str, filename: str = \"scraped_content.html\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_html)\n",
    "    print(f\"Raw HTML content has been written to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query_web_scraper initiates the playwright response and write_raw_html_to_file is self explanatory.Â \n",
    "\n",
    "Now you can implement the multistep tool call to have the model initiate the scraping and use the results to create a structured JSON response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'query_web_scraper' was called with the URL: https://ollama.com/blog/tool-support\n",
      "Raw HTML content has been written to scraped_content.html\n",
      "{\n",
      "  \"title\": \"Tool Support\",\n",
      "  \"description\": \"Bring your ideas to life with Ollama's tool support.\",\n",
      "  \"section\": {\n",
      "    \"h2\": \"What is a tool?\",\n",
      "    \"p1\": \"A tool in Ollama allows you to take action directly on the model output. Each time a model processes an input, it can return one or more tools that can be used to further process the output.\",\n",
      "    \"code\": [\n",
      "      {\n",
      "        \"lang\": \"python\",\n",
      "        \"line\": \"ollama=ollama.Ollama('https://api.ollama.com/v1',\\n\\tmessages=messages,\\n\\ttools=tools,\\n)\"\n",
      "      }\n",
      "    ],\n",
      "    \"h3\": \"Full examples\",\n",
      "    \"ul\": [\n",
      "      {\n",
      "        \"li\": [\n",
      "          {\n",
      "            \"href\": \"https://github.com/ollama/ollama-python/blob/main/examples/tools/main.py\",\n",
      "            \"text\": \"Python\"\n",
      "          },\n",
      "          {\n",
      "            \"href\": \"https://github.com/ollama/ollama-js/blob/main/examples/tools/tools.ts\",\n",
      "            \"text\": \"JavaScript\"\n",
      "          }\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"h3\": \"Future improvements\",\n",
      "    \"ul\": [\n",
      "      {\n",
      "        \"li\": [\n",
      "          \"Streaming tool calls: stream tool calls back to begin taking action faster when multiple tools are returned\",\n",
      "          \"Tool choice: force a model to use a tool\"\n",
      "        ]\n",
      "      }\n",
      "    ],\n",
      "    \"h3\": \"Let's build together\",\n",
      "    \"p2\": [\n",
      "      \"We are so excited to bring you tool support, and see what you build with it!\",\n",
      "      \"If you have any feedback, please do not hesitate to tell us either in our <a href=\\\"https://discord.com/invite/ollama\\\">Discord</a> or via <a href=\\\"mailto:hello@ollama.com\\\">hello@ollama.com</a>\"\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and messages\n",
    "model = 'llama3.1'\n",
    "\n",
    "# Revised system message focused on structured JSON output\n",
    "system_message = {\n",
    "    'role': 'system',\n",
    "    'content': 'You are an AI assistant specialized in processing web content and returning structured JSON data. Always provide your response as valid, well-formatted JSON without any additional text or comments. Focus on extracting and organizing the most relevant information from websites, including main sections, key services or products, and primary navigation links.'\n",
    "}\n",
    "\n",
    "# User message requesting the scraping of content\n",
    "user_message = {\n",
    "    'role': 'user',\n",
    "    'content': 'Please scrape the content of https://ollama.com/blog/tool-support and provide a structured JSON response of all the titles and links on the page. After scraping, focus on the most important and relevant information.'\n",
    "}\n",
    "\n",
    "# Initialize conversation with the system message and user query\n",
    "messages = [system_message, user_message]\n",
    "\n",
    "# First API call: Send the query and function description to the model\n",
    "response = ollama.chat(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    tools=[\n",
    "        {\n",
    "            'type': 'function',\n",
    "            'function': {\n",
    "                'name': 'query_web_scraper',\n",
    "                'description': 'Scrapes the content of a web page and returns the structured JSON object with titles, articles, and associated links.',\n",
    "                'parameters': {\n",
    "                    'type': 'object',\n",
    "                    'properties': {\n",
    "                        'url': {\n",
    "                            'type': 'string',\n",
    "                            'description': 'The URL of the web page to scrape.',\n",
    "                        },\n",
    "                    },\n",
    "                    'required': ['url'],\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Append the model's response to the existing messages\n",
    "messages.append(response['message'])\n",
    "\n",
    "# Check if the model decided to use the provided function\n",
    "if not response['message'].get('tool_calls'):\n",
    "    print(\"The model didn't use the function. Its response was:\")\n",
    "    print(response['message']['content'])\n",
    "else:\n",
    "    # Process function calls made by the model\n",
    "    scraped_data = None\n",
    "    available_functions = {'query_web_scraper': query_web_scraper}\n",
    "\n",
    "    for tool in response['message']['tool_calls']:\n",
    "        function_name = tool['function']['name']\n",
    "        function_to_call = available_functions[function_name]\n",
    "        function_args = tool['function']['arguments']\n",
    "        scraped_data = await function_to_call(function_args['url'])  # Use await for async function call\n",
    "        \n",
    "        print(f\"Function '{function_name}' was called with the URL: {function_args['url']}\")\n",
    "        \n",
    "        # Write raw HTML to file\n",
    "        await write_raw_html_to_file(scraped_data['raw_html'])\n",
    "        \n",
    "        # Add function response to the conversation\n",
    "        messages.append({\n",
    "            'role': 'function',\n",
    "            'name': function_name,\n",
    "            'content': json.dumps(scraped_data),\n",
    "        })\n",
    "\n",
    "    if scraped_data:\n",
    "        # Additional instruction to ensure proper use of scraped data\n",
    "        additional_instruction = {\n",
    "            'role': 'user',\n",
    "            'content': f\"\"\"Here's the scraped data from the website:\n",
    "            \n",
    "            {json.dumps(scraped_data, indent=2)}\n",
    "            \n",
    "            Using this scraped data, create a structured JSON response that includes only the most relevant and important information from the website.\n",
    "            Ignore head section. Focus on the main body section. Do not include HTML tags or unnecessary details.\n",
    "            Ensure your response is in valid JSON format without any additional text or comments. Make sure you dont return empty JSON. The structure should match the information you are scraping.\"\"\"\n",
    "        }\n",
    "        messages.append(additional_instruction)\n",
    "\n",
    "        # Final API call: Get structured JSON response from the model\n",
    "        final_response = ollama.chat(model=model, messages=messages)\n",
    "        print(final_response['message']['content'])\n",
    "    else:\n",
    "        print(\"No data was scraped. Unable to proceed with creating a structured JSON response.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
